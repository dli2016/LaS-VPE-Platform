##############################################################################
# This file contains system-wise default properties. Properties
# specified here can be overridden by application-specific properties.
##############################################################################
########################    Zookeeper configuration   ########################
zk.connect=gpu-task-nod1:2181,gpu-task-nod2:2181,gpu1608:2181
zk.session.timeout=30000
zk.connection.timeout=30000
##############################################################################
########################      Kafka configuration     ########################
kafka.bootstrap.servers=gpu-task-nod1:9092,gpu-task-nod2:9092,gpu1608:9092
kafka.partitions=10
kafka.replication.factor=2
kafka.fetch.max.size=1048576
kafka.send.max.size=1048576
kafka.request.timeout.ms=120000
kafka.fetch.timeout.ms=60000
# If executors are on the same hosts as Kafka brokers, use PreferBrokers.
# Otherwise, use PreferConsistent.
# PreferFixed is currently not supported in this platform.
kafka.location.strategy=PreferBrokers
##############################################################################
########################      Spark configuration     ########################
# Checkpoint directory (on HDFS)
spark.checkpoint.dir=checkpoint
# spark.master=local[*]
spark.master=yarn-cluster
# The maximum number of messages per second that each partition will accept in
# the direct Kafka input stream. Not set by default (leave it as 0), then the
# rate is not limited. For the CPU consuming applications (such as the
# pedestrian tracking app, it is recommended to set to the core numbers of
# the tracking worker with least cores.
spark.streaming.kafka.maxRatePerPartition=0
# A YARN node label expression that restricts the set of nodes AM will be
# scheduled on.
# Only versions of YARN greater than or equal to 2.6 support
# node label expressions, so when running against earlier versions, this
# property will be ignored.
# To enable label-based scheduling, see
# https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/NodeLabel.html
spark.yarn.am.nodeLabelExpression=
##############################################################################
######################  LaS-VPE-Platform configuration  ######################
# The path the jar package of LaS-VPE Platform is stored at.
vpe.platform.jar=bin/las-vpe-platform-0.0.1-full.jar
# The metadata saving directory. This should be an absolute path without the
# "hdfs://" or "har://" tag (e.g. /metadata or /user/labadmin/metadata).
vpe.metadata.dir=/user/vpe.cripac/metadata_ssd_tracker
# Duration for buffering results (ms).
vpe.buf.duration=600000
# Duration of batch (ms).
# Batches of data should be processed as fast as they are being generated.
# The batch processing time should be less than the batch interval.
vpe.batch.duration=1000
# Number of partitions when doing repartition.
# -1 means do not do repartition. 0 means using default parallelism of Spark.
vpe.repartition=-1
# Whether to enable task controller.
vpe.task.controller.enable=1
##############################################################################
########################  Resources for application   ########################
# Number of executor instances (can override configuration in spark-defaults.conf).
num.executors=3
# Memory per executor (e.g. 1000M, 2G) (Default: 1G)
executor.memory=6G
# Number of cores per executor (Default: 1)
executor.cores=2
# Memory for driver (e.g. 1000M, 2G) (Default: 1024 Mb)
driver.memory=2G
# The hadoop queue to use for allocation requests (Default: 'default')
hadoop.queue=default
##############################################################################
############################ CaffeBytedeco configuration #############################
# GPUs to use for Caffe, separated by commas. -1 means using CPU.
caffe.gpu=-1
##############################################################################
